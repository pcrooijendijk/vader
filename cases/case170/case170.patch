--- _generate.py        2025-04-29 18:44:17.357423566 +0530
+++ _generate-new.py    2025-04-29 18:44:31.237420920 +0530
@@ -22,6 +22,11 @@

 from . import _errors as errors
 from ._stream import TorrentFileStream
+import threading
+from contextlib import contextmanager
+
+# Global lock for shared queue operations
+QUEUE_LOCK = threading.Lock()

 QUEUE_CLOSED = object()

@@ -43,6 +48,8 @@
     :class:`threading.Thread` subclass that re-raises any exceptions from the
     thread when joined
     """
+    def __init__(self, *args, **kwargs):
+        self._thread_lock = threading.Lock()

     def __init__(self, name, worker, start=True, fail_ok=False):
         self._exception = None
@@ -95,14 +102,32 @@
     :class:`Worker` subclass that reads files in pieces and pushes them to a
     queue
     """
+def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._stream_lock = threading.Lock()

-    def __init__(self, *, torrent, queue_size, path=None):
-        self._torrent = torrent
-        self._path = path
-        self._piece_queue = queue.Queue(maxsize=queue_size)
-        self._stop = False
-        self._memory_error_timestamp = -1
-        super().__init__(name='reader', worker=self._push_pieces)
+    def _handle_oom(self, exception):
+        with QUEUE_LOCK:
+            now = time_monotonic()
+            time_diff = now - self._memory_error_timestamp
+            if time_diff >= 0.1:
+                old_maxsize = self._piece_queue.maxsize
+                new_maxsize = max(1, int(old_maxsize * 0.9))
+                if new_maxsize != old_maxsize:
+                    _debug(f'{_thread_name()}: Reducing piece_queue.maxsize to {new_maxsize}')
+                    self._piece_queue.maxsize = new_maxsize
+                    self._memory_error_timestamp = now
+                else:
+                    raise errors.ReadError(errno.ENOMEM, exception)
+
+    def stop(self):
+        with self._thread_lock:
+            if not self._stop:
+                _debug(f'{_thread_name()}: {type(self).__name__}: Setting stop flag')
+                self._stop = True
+                with self._stream_lock:
+                    if hasattr(self, '_stream'):
+                        self._stream.close()

     def _push_pieces(self):
         stream = TorrentFileStream(self._torrent)
@@ -137,26 +162,6 @@
         # _debug(f'{_thread_name()}: Pushing #{piece_index}: {filepath}: {_pretty_bytes(piece)}, {exceptions!r}')
         self._piece_queue.put((piece_index, filepath, piece, exceptions))

-    def _handle_oom(self, exception):
-        # Reduce piece_queue.maxsize by 1 every 100ms until the MemoryErrors stop
-        now = time_monotonic()
-        time_diff = now - self._memory_error_timestamp
-        if time_diff >= 0.1:
-            old_maxsize = self._piece_queue.maxsize
-            new_maxsize = max(1, int(old_maxsize * 0.9))
-            if new_maxsize != old_maxsize:
-                _debug(f'{_thread_name()}: Reducing piece_queue.maxsize to {new_maxsize}')
-                self._piece_queue.maxsize = new_maxsize
-                self._memory_error_timestamp = now
-            else:
-                raise errors.ReadError(errno.ENOMEM, exception)
-
-    def stop(self):
-        """Stop reading and close the piece queue"""
-        if not self._stop:
-            _debug(f'{_thread_name()}: {type(self).__name__}: Setting stop flag')
-            self._stop = True
-
     @property
     def piece_queue(self):
         """
@@ -173,7 +178,9 @@
     the resulting hash to :attr:`hash_queue`
     """

-    def __init__(self, hasher_threads, piece_queue):
+    def __init__(self, *args, **kwargs):
+        self._shutdown_lock = threading.Lock()
+        self._hasher_locks = {name: threading.Lock() for name in [f'hasher{i}' for i in range(1, kwargs['hasher_threads']+1)]}
         self._piece_queue = piece_queue
         self._hash_queue = queue.Queue()
         self._finalize_event = threading.Event()
@@ -215,31 +222,41 @@
             hasher.start(fail_ok=True)

     def _hasher_thread(self, is_vital=True):
-        piece_queue = self._piece_queue
-        handle_piece = self._handle_piece
         while True:
-            # _debug(f'{_thread_name()}: Waiting for next task')
             try:
-                task = piece_queue.get(timeout=0.5)
+                with QUEUE_LOCK:
+                    task = self._piece_queue.get(timeout=0.5)
+
+                if task is QUEUE_CLOSED:
+                    with QUEUE_LOCK:
+                        self._piece_queue.put(QUEUE_CLOSED)
+                    with self._shutdown_lock:
+                        self._finalize_event.set()
+                    break
+
+                self._handle_piece(*task)
+
             except queue.Empty:
                 if not is_vital:
                     _debug(f'{_thread_name()}: I am bored, byeee!')
                     break
-                else:
-                    _debug(f'{_thread_name()}: I am bored, but needed.')
-            else:
-                if task is QUEUE_CLOSED:
-                    _debug(f'{_thread_name()}: piece_queue is closed')
-                    # Repeat QUEUE_CLOSED to the next sibling. This ensures
-                    # there is always one more QUEUE_CLOSED queued than running
-                    # threads. Otherwise, one thread might consume multiple
-                    # QUEUE_CLOSED and leave other threads running forvever.
-                    piece_queue.put(QUEUE_CLOSED)
-                    # Signal janitor to initiate shutdown procedure
-                    self._finalize_event.set()
-                    break
-                else:
-                    handle_piece(*task)
+                continue
+
+    def _janitor_thread(self):
+        while True:
+            finalization_initiated = self._finalize_event.wait(timeout=1.0)
+            if finalization_initiated:
+                with self._shutdown_lock:
+                    self._wait_for_hashers()
+                    with QUEUE_LOCK:
+                        self._hash_queue.put(QUEUE_CLOSED)
+                break
+            # Thread-safe hasher pruning
+            with self._shutdown_lock:
+                for hasher in tuple(self._hashers):
+                    if not hasher.is_running:
+                        _debug(f'{_thread_name()}: Pruning {hasher.name}')
+                        self._hashers.remove(hasher)

     def _handle_piece(self, piece_index, filepath, piece, exceptions):
         if exceptions:
@@ -256,24 +273,6 @@
             # _debug(f'{_thread_name()}: Nothing to hash for #{piece_index}: {piece!r}')
             self._hash_queue.put((piece_index, filepath, None, ()))

-    def _janitor_thread(self):
-        while True:
-            _debug(f'{_thread_name()}: Waiting for finalize event')
-            finalization_initiated = self._finalize_event.wait(timeout=1.0)
-            if finalization_initiated:
-                self._wait_for_hashers()
-                _debug(f'{_thread_name()}: Closing hash queue')
-                self._hash_queue.put(QUEUE_CLOSED)
-                break
-
-            else:
-                # Remove terminated idle hashers
-                for hasher in tuple(self._hashers):
-                    if not hasher.is_running:
-                        _debug(f'{_thread_name()}: Pruning {hasher.name}')
-                        self._hashers.remove(hasher)
-
-        _debug(f'{_thread_name()}: Terminating')

     def _wait_for_hashers(self):
         while True:
@@ -348,38 +347,16 @@
         return self.hashes

     def _collect(self, piece_index, filepath, piece_hash, exceptions):
-        # _debug(f'{_thread_name()}: Collecting #{piece_index}: {_pretty_bytes(piece_hash)}, {exceptions}')
-
-        # Remember which pieces where hashed to count them and for sanity checking
-        assert piece_index not in self._pieces_seen
-        self._pieces_seen.append(piece_index)
-
-        # Collect piece
-        if not exceptions and piece_hash:
-            self._hashes_unsorted.append((piece_index, piece_hash))
-
-        # If there is no callback, raise first exception
-        if exceptions and not self._callback:
-            raise exceptions[0]
-
-        # Report progress/exceptions and allow callback to cancel
-        if self._callback:
-            # _debug(f'{_thread_name()}: Collector callback: {self._callback}')
-            maybe_cancel = self._callback(
-                piece_index, len(self._pieces_seen), self._pieces_total,
-                filepath, piece_hash, exceptions,
-            )
-            # _debug(f'{_thread_name()}: Collector callback return value: {maybe_cancel}')
-            if maybe_cancel is not None:
-                self._cancel()
+        with threading.Lock():
+            assert piece_index not in self._pieces_seen
+            self._pieces_seen.append(piece_index)
+
+            if not exceptions and piece_hash:
+                self._hashes_unsorted.append((piece_index, piece_hash))

     def _cancel(self):
-        # NOTE: We don't need to stop HasherPool or Collector.collect() because
-        #       they will stop when Reader._push_pieces() pushes QUEUE_CLOSED.
-        #       They will process the pieces in the queue, but that shouldn't
-        #       take long unless the Reader's queue size is too big.
-        self._reader.stop()
-
+        with threading.Lock():
+            self._reader.stop()
     def _finalize(self):
         _debug(f'{_thread_name()}: Joining {self._reader}')
         self._reader.join()
@@ -538,3 +515,12 @@
                 pass

         raise RuntimeError(f'Failed to get path from {exception!r}')
+
+@contextmanager
+def atomic_file_operation(filepath):
+    lock = threading.Lock()
+    with lock:
+        if os.path.exists(filepath):
+            yield open(filepath, 'rb')
+        else:
+            raise FileNotFoundError(f"File disappeared: {filepath}")